{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from models import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb5e00c4610>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2020)\n",
    "torch.manual_seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of training dataset and validation dataset\n",
    "train_csv_path = '/home/tu-mingzhiwang/data/chexpert/v1.0/train.csv'\n",
    "valid_csv_path = '/home/tu-mingzhiwang/data/chexpert/v1.0/valid.csv'\n",
    "\n",
    "dir_path = '/home/tu-mingzhiwang/data/chexpert/v1.0/'\n",
    "\n",
    "# store model parameters\n",
    "model_path = '/home/tu-mingzhiwang/group/igloo/CheXpert_wang/checkpoints_new_dense121' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pre-processing, only use frontal images.\n",
    "train_csv = pd.read_csv(train_csv_path, sep=',').fillna(0)\n",
    "valid_csv = pd.read_csv(valid_csv_path, sep=',').fillna(0)\n",
    "train_csv = train_csv[~train_csv[train_csv.columns[3]].str.contains(\"Lateral\")]\n",
    "valid_csv = valid_csv[~valid_csv[valid_csv.columns[3]].str.contains(\"Lateral\")]\n",
    "\n",
    "#this image is lost\n",
    "pa = 'CheXpert-v1.0/train/patient40761/study1/view1_frontal.jpg'\n",
    "train_csv = train_csv[~train_csv[train_csv.columns[0]].str.contains(pa)]\n",
    "\n",
    "# drop other information \n",
    "train_csv = train_csv.drop(['Sex', 'Age', 'Frontal/Lateral', 'AP/PA'], axis=1)\n",
    "valid_csv = valid_csv.drop(['Sex', 'Age', 'Frontal/Lateral', 'AP/PA'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data\n",
    "normTransform = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# chest is not always in the central of image, so i will resize it as 240 then random crop a \n",
    "# size of (224,224)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((240, 240)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomRotation(7),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    normTransform\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset and dataloader\n",
    "# deal uncertain label with np.random.uniform(0.55, 0.85)\n",
    "\n",
    "class Datasets(Dataset):\n",
    "    \n",
    "    def __init__(self, state, transform=None):\n",
    "        self.path = state.iloc[1:,0].values\n",
    "        labels = state.iloc[1:, 1:].values\n",
    "        for row in range(labels.shape[0]):\n",
    "            for column in range(labels.shape[1]):\n",
    "                if labels[row][column] != 0.0 and labels[row][column] != 1.0:\n",
    "                    labels[row][column] = round(np.random.uniform(0.55, 0.85), 2)\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.path[index]\n",
    "        img_path = img_path.replace('CheXpert-', 'chexpert/')\n",
    "        dir_path = '/home/tu-mingzhiwang/data'\n",
    "        image = Image.open(os.path.join(dir_path, img_path)).convert('RGB')\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, torch.FloatTensor(label)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.path.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Datasets\n",
    "train_data = Datasets(train_csv, train_transform)\n",
    "val_data = Datasets(valid_csv, test_transform)\n",
    "\n",
    "# put it into dataloader, batchsize depends on the model\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "# we can use DenseNet121, DenseNet121_with_dropout, Combined_model_1, Efficientnet, Combined_model_2\n",
    "model = Efficientnet().cuda()\n",
    "\n",
    "\n",
    "#choose Loss, Optimizer\n",
    "loss = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999))\n",
    "learning_rate_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly',\n",
       "       'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation',\n",
       "       'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion',\n",
       "       'Pleural Other', 'Fracture', 'Support Devices'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_name = train_csv.columns[1:].values\n",
    "labels_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 2, 6, 5, 10]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']\n",
    "target_index = [list(labels_name).index(item) for item in target]\n",
    "target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scores = []\n",
    "target_score_averages = []\n",
    "\n",
    "# use tensorboard to record\n",
    "writer = SummaryWriter()\n",
    "\n",
    "num_epoch = 40\n",
    "best_score = 0.0\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_star = time.time()\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_pred = torch.FloatTensor().cuda()     # Tensor stores prediction values\n",
    "    val_label = torch.FloatTensor().cuda()       # Tensor stores true values\n",
    "    \n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_prd = model(data[0].cuda())\n",
    "        batch_loss = loss(train_prd, data[1].cuda())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # tensorboard for batch loss\n",
    "        train_loss += batch_loss.item()\n",
    "        index_train = len(train_loader) * epoch + i\n",
    "        writer.add_scalar('batch_loss', batch_loss.item() , index_train)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,data in enumerate(val_loader):\n",
    "            prd = model(data[0].cuda())\n",
    "            batch_loss = loss(prd, data[1].cuda())\n",
    "\n",
    "            val_loss += batch_loss.item()\n",
    "            val_pred = torch.cat((val_pred, prd), 0)\n",
    "            val_label = torch.cat((val_label, data[1].cuda()), 0)\n",
    "      \n",
    "    learning_rate_scheduler.step()\n",
    "    \n",
    "    #tensorboard for score and train loss\n",
    "    target_score = [roc_auc_score(val_label.cpu().numpy()[:, i], val_pred.detach().cpu().numpy()[:, i],average='macro', multi_class='ovo') for i in target_index]\n",
    "    target_scores.append(target_score)\n",
    "    target_score_average = roc_auc_score(val_label.cpu().numpy()[:, target_index], val_pred.detach().cpu().numpy()[:, target_index],average='macro', multi_class='ovo')\n",
    "    target_score_averages.append(target_score_average)\n",
    "\n",
    "    \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "\n",
    "    best_score = target_score_average.round(5)\n",
    "    torch.save({'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict()}, os.path.join(model_path, f'epoch_{epoch}_score_{best_score}.pth'))\n",
    "        \n",
    "    writer.add_scalars('train/val for epochs', {'train_loss':np.array(train_loss), \\\n",
    "                                               'val_target':np.array(target_score_average),\\\n",
    "                                               'val_loss': np.array(val_loss)}, epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch - wang",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
